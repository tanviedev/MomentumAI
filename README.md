ğŸ§  Content Insight Engine (MVP)

An AI-powered engine that analyzes social media content performance and generates actionable insights using a local LLM (Phi via Ollama).

Users do not enter JSON.
They simply select a post link, and the system:

fetches structured performance data

runs LLM reasoning

returns human-readable insights

âœ¨ Features

ğŸ”— Link-based analysis (Instagram, YouTube, LinkedIn, etc.)

ğŸ§  Local LLM inference using Ollama (Phi)

âš¡ FastAPI backend

ğŸ¨ React + Vite frontend

ğŸ§ª MVP-friendly in-memory content database

ğŸ”’ Safe JSON-only LLM responses

ğŸš€ Ready for deployment

ğŸ—‚ï¸ Project Structure
digiBot/
â”‚
â”œâ”€â”€ contextEngine/
â”‚   â””â”€â”€ backend/
â”‚       â”œâ”€â”€ main.py
â”‚       â”œâ”€â”€ llm/
â”‚       â”‚   â”œâ”€â”€ insight_engine.py
â”‚       â”‚   â””â”€â”€ prompt.txt
â”‚       â”œâ”€â”€ data/
â”‚       â”‚   â””â”€â”€ content_store.py
â”‚       â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ App.jsx
â”‚   â”‚   â”œâ”€â”€ api.js
â”‚   â”‚   â”œâ”€â”€ main.jsx
â”‚   â”‚   â”œâ”€â”€ index.css
â”‚   â”‚   â””â”€â”€ App.css
â”‚   â””â”€â”€ package.json
â”‚
â””â”€â”€ README.md

ğŸ§° Prerequisites
1. System

Python 3.10+

Node.js 18+

Git

Windows / macOS / Linux

2. Ollama (Local LLM)

Install Ollama from:
ğŸ‘‰ https://ollama.com

Pull Phi model:

ollama pull phi


Verify:

ollama run phi

ğŸ§  Backend Setup (FastAPI)
1. Go to backend
cd contextEngine/backend

2. Create virtual environment (recommended)
python -m venv venv
venv\Scripts\activate   # Windows

3. Install dependencies
pip install fastapi uvicorn requests pydantic

4. Start the backend server
uvicorn main:app --reload


Backend will run at:

http://127.0.0.1:8000

5. Available API Routes
Method	Route	Description
GET	/links	Returns all available post links
POST	/analyze/link	Analyzes selected link

Example request:

{
  "link": "https://instagram.com/p/abc123"
}

ğŸ§  LLM Insight Engine

Model: phi (via Ollama)

Runs locally on CPU

Strict JSON output enforcement

Safe fallback if model returns invalid output

Prompt lives in:

backend/llm/prompt.txt


Inference logic:

backend/llm/insight_engine.py

ğŸ—ƒï¸ Content Database (MVP)

Current MVP uses an in-memory Python dictionary.

Location:

backend/data/content_store.py


Why?

No DB setup required

Easy to test and iterate

Perfect for demos & validation

âš ï¸ No deployment issues with this for MVP.

Future upgrade:

PostgreSQL / Supabase / Firebase

ğŸ¨ Frontend Setup (React + Vite)
1. Go to frontend
cd frontend

2. Install dependencies
npm install

3. Start frontend
npm run dev


Frontend runs at:

http://localhost:5173

ğŸ–¥ï¸ Frontend Flow

User selects a post link from dropdown

Clicks Analyze

Frontend calls backend

LLM generates insights

Results are rendered cleanly

âœ… Example Output
AI Insight

Failure Reason:
intent_match is misaligned, message and audience expectation differ

Success Driver:
Timing quality was good

Recommended Actions:
- Align message with audience intent
- Improve hook strength

Confidence: 80%

ğŸ§ª Common Issues & Fixes
âŒ 404 on /analyze

âœ… Use:

POST /analyze/link

âŒ Ollama port error (11434 in use)

âœ… Ollama is already running
DO NOT run ollama serve again.

âŒ Frontend shows â€œSomething went wrongâ€

Check backend is running

Check correct API route

Check CORS enabled

ğŸš€ Deployment (Next Steps)

Ready for:

Vercel (Frontend)

Railway / Render (Backend)

Dockerization

Real-time scraping

Auth + user uploads

ğŸ§  Roadmap

ğŸ” Auto-scrape link metadata

ğŸ“Š Engagement visualizations

ğŸ§  Multi-pass LLM reasoning

ğŸ§ª A/B hook analysis

ğŸ§¬ Personalization by platform

â¤ï¸ Built With

FastAPI

React + Vite

Ollama

Phi LLM

Pure caffeine & persistence â˜•ğŸ”¥