# AI Engagement Analyst

**AI Engagement Analyst** is a Decision Intelligence system that turns content analytics into clear, actionable coaching.

Instead of showing raw metrics, the system explains:
- *Why* a post performed the way it did
- *What* worked and what didnâ€™t
- *What to do next*

The goal is to reduce decision fatigue and guide users toward the **single most useful next action**.

---

## ğŸš€ What This Project Does

- Analyzes content performance data
- Identifies posts that need attention
- Explains engagement patterns in plain language
- Recommends what to fix next
- Provides a calm, minimal dashboard UX

This is **not** a traditional analytics dashboard.  
It is a **Decision Intelligence assistant**.

---

## ğŸ§  Core Features

- **Needs Attention Queue**  
  Only shows posts that require action, ranked by priority.

- **AI Insight Panel**  
  One focused insight at a time â€” no overload.

- **Per-Content Drilldown**  
  Click any post to understand *why* it behaved the way it did.

- **â€œNext Post to Improveâ€ Button**  
  Removes choice paralysis by telling the user where to start.

---

## ğŸ—‚ Project Structure

```text
ai_engagement_analyst/
â”œâ”€â”€ api/
â”‚   â””â”€â”€ dashboard_api.py
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ engagement_dataset.csv
â”‚
â”œâ”€â”€ features/
â”‚   â””â”€â”€ feature_engineering.py
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ classifier.py
â”‚   â”œâ”€â”€ reasoning_engine.py
â”‚   â””â”€â”€ performance_classifier.pkl
â”‚
â”œâ”€â”€ insights/
â”‚   â””â”€â”€ insight_generator.py
â”‚
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ config.py
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ index.html
â”‚   â”œâ”€â”€ style.css
â”‚   â””â”€â”€ app.js
â”‚
â”œâ”€â”€ train.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md



âš™ï¸ Setup Instructions
1ï¸âƒ£ Create environment (recommended)
python -m venv venv
source venv/bin/activate   # macOS/Linux
# venv\Scripts\activate    # Windows

2ï¸âƒ£ Install dependencies
pip install -r requirements.txt

ğŸ§ª Train the Model (one-time): This generates performance_classifier.pkl.
python train.py

You should see:
Model trained and saved to models/performance_classifier.pkl

â–¶ï¸ Run the Backend
From the project root:
uvicorn api.dashboard_api:app --reload
Backend runs at:
http://127.0.0.1:8000

Useful endpoints:
/dashboard
/content/{content_id}
/next

ğŸŒ Run the Frontend
Do not open index.html directly.
Serve it via HTTP:
cd frontend
python -m http.server 5500
Open in browser:
http://127.0.0.1:5500 OR right-click 'index.html' and click: open with live server


1.) uvicorn api.dashboard_api:app --reload
This does one critical job:
Starts a server at http://127.0.0.1:8000
Exposes routes like:
/dashboard
/content/{id}
/next
Think of this as turning the brain on.

If this is not running:
fetch("http://127.0.0.1:8000/dashboard") has nowhere to go
The browser silently fails
UI looks â€œstaticâ€

2.) index.html (served via HTTP)
This does a different job:
Loads UI
Runs JavaScript
Calls the backend using fetch()
Think of this as the eyes and hands.

If this runs without the backend:
UI loads
JS runs
API calls fail
No data appears









ğŸ§  Content Insight Engine (MVP)

An AI-powered engine that analyzes social media content performance and generates actionable insights using a local LLM (Phi via Ollama).

Users do not enter JSON.
They simply select a post link, and the system:

fetches structured performance data

runs LLM reasoning

returns human-readable insights

âœ¨ Features

ğŸ”— Link-based analysis (Instagram, YouTube, LinkedIn, etc.)

ğŸ§  Local LLM inference using Ollama (Phi)

âš¡ FastAPI backend

ğŸ¨ React + Vite frontend

ğŸ§ª MVP-friendly in-memory content database

ğŸ”’ Safe JSON-only LLM responses

ğŸš€ Ready for deployment

ğŸ—‚ï¸ Project Structure
digiBot/
â”‚
â”œâ”€â”€ contextEngine/
â”‚   â””â”€â”€ backend/
â”‚       â”œâ”€â”€ main.py
â”‚       â”œâ”€â”€ llm/
â”‚       â”‚   â”œâ”€â”€ insight_engine.py
â”‚       â”‚   â””â”€â”€ prompt.txt
â”‚       â”œâ”€â”€ data/
â”‚       â”‚   â””â”€â”€ content_store.py
â”‚       â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ App.jsx
â”‚   â”‚   â”œâ”€â”€ api.js
â”‚   â”‚   â”œâ”€â”€ main.jsx
â”‚   â”‚   â”œâ”€â”€ index.css
â”‚   â”‚   â””â”€â”€ App.css
â”‚   â””â”€â”€ package.json
â”‚
â””â”€â”€ README.md

ğŸ§° Prerequisites
1. System

Python 3.10+

Node.js 18+

Git

Windows / macOS / Linux

2. Ollama (Local LLM)

Install Ollama from:
ğŸ‘‰ https://ollama.com

Pull Phi model:

ollama pull phi


Verify:

ollama run phi

ğŸ§  Backend Setup (FastAPI)
1. Go to backend
cd contextEngine/backend

2. Create virtual environment (recommended)
python -m venv venv
venv\Scripts\activate   # Windows

3. Install dependencies
pip install fastapi uvicorn requests pydantic

4. Start the backend server
uvicorn main:app --reload


Backend will run at:

http://127.0.0.1:8000

5. Available API Routes
Method	Route	Description
GET	/links	Returns all available post links
POST	/analyze/link	Analyzes selected link

Example request:

{
  "link": "https://instagram.com/p/abc123"
}

ğŸ§  LLM Insight Engine

Model: phi (via Ollama)

Runs locally on CPU

Strict JSON output enforcement

Safe fallback if model returns invalid output

Prompt lives in:

backend/llm/prompt.txt


Inference logic:

backend/llm/insight_engine.py

ğŸ—ƒï¸ Content Database (MVP)

Current MVP uses an in-memory Python dictionary.

Location:

backend/data/content_store.py


Why?

No DB setup required

Easy to test and iterate

Perfect for demos & validation

âš ï¸ No deployment issues with this for MVP.

Future upgrade:

PostgreSQL / Supabase / Firebase

ğŸ¨ Frontend Setup (React + Vite)
1. Go to frontend
cd frontend

2. Install dependencies
npm install

3. Start frontend
npm run dev


Frontend runs at:

http://localhost:5173

ğŸ–¥ï¸ Frontend Flow

User selects a post link from dropdown

Clicks Analyze

Frontend calls backend

LLM generates insights

Results are rendered cleanly

âœ… Example Output
AI Insight

Failure Reason:
intent_match is misaligned, message and audience expectation differ

Success Driver:
Timing quality was good

Recommended Actions:
- Align message with audience intent
- Improve hook strength

Confidence: 80%

ğŸ§ª Common Issues & Fixes
âŒ 404 on /analyze

âœ… Use:

POST /analyze/link

âŒ Ollama port error (11434 in use)

âœ… Ollama is already running
DO NOT run ollama serve again.

âŒ Frontend shows â€œSomething went wrongâ€

Check backend is running

Check correct API route

Check CORS enabled

ğŸš€ Deployment (Next Steps)

Ready for:

Vercel (Frontend)

Railway / Render (Backend)

Dockerization

Real-time scraping

Auth + user uploads

ğŸ§  Roadmap

ğŸ” Auto-scrape link metadata

ğŸ“Š Engagement visualizations

ğŸ§  Multi-pass LLM reasoning

ğŸ§ª A/B hook analysis

ğŸ§¬ Personalization by platform

â¤ï¸ Built With

FastAPI

React + Vite

Ollama

Phi LLM

Pure caffeine & persistence â˜•ğŸ”¥